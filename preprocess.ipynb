{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將爬蟲出來的句子做前處理，像是刪除停用詞等等\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "#https://www.kaggle.com/cpmpml/spell-checker-using-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spellchecker\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#大眾形容詞加入停用詞\n",
    "adj_list=[\"good\",\"great\",\"best\",\"nice\",\"bad\",\"worse\",\"worst\",\"well\",\"cool\",\"restaurant\",\"AM\",\"pm\",\"PM\",\"true\",\"false\"]\n",
    "#大寫的停用詞加入\n",
    "stop_words_list=stopwords.words('english')+adj_list\n",
    "for x in range(len(stop_words_list)):\n",
    "    stop_words_list[x]=stop_words_list[x].title()\n",
    "#全部的停用詞\n",
    "stop_words = set(stopwords.words('english')+stop_words_list+adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(string):\n",
    "    #去除英文以外\n",
    "    string = re.sub(r'[^A-Za-z]+', ' ', string)\n",
    "    #去除前後空格\n",
    "    return string\n",
    "def stopword_removal_append(string):    \n",
    "    words = string.split()\n",
    "    temp=0\n",
    "    for r in words: \n",
    "        if not r in stop_words:\n",
    "            r = correction(r)\n",
    "            with open('./review_sets/review'+str(count)+'.txt','a', encoding = 'UTF8') as f1 : \n",
    "                f1.write(r+\" \") \n",
    "                f1.close()\n",
    "                temp=1\n",
    "    if temp==0:\n",
    "        with open('./review_sets/review'+str(count)+'.txt','a', encoding = 'UTF8') as f1 :\n",
    "            f1.write(\"others\")\n",
    "            f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "reviews = 995 #要隨著reviews筆數更改\n",
    "for x in range(1,reviews+1): #總共1~reviews+1 \n",
    "    df = pd.read_csv('original_review_sets/review'+str(x)+'.txt', sep='@', header=None,quoting=3)#quoting 防止有\"會EOF\n",
    "    string = df.values.tolist()[0][0]\n",
    "    string = preprocess(string)\n",
    "    stopword_removal_append(string)\n",
    "    count+=1\n",
    "#遇到會錯誤的那筆資料，請將original_review的那筆資料改為others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
